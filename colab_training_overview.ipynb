{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rhodes-CS-comp377/comp377-colab-rl-training-example/blob/main/colab_training_overview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook provides an **Overview** and **Quick Start Guide** to help you begin applying \"state-of-the-art\" RL algorithms to solve complex environments (such as [Atari](https://gymnasium.farama.org/environments/atari/) and [Mujoco](https://gymnasium.farama.org/environments/mujoco/)).\n",
        "\n",
        "\n",
        "We will be using the algorithm implementations provided in the [Stable Baselines 3](https://stable-baselines3.readthedocs.io/) library. These implementations allow us to train RL agents on any environment that conforms to the [Gymnasium](https://gymnasium.farama.org/) (formerly, \"OpenAI Gym\") environment specification.\n",
        "\n",
        "This includes [custom environments](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html) that you might create yourself."
      ],
      "metadata": {
        "id": "N1WAQKA9dCUm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sources of Additional Information"
      ],
      "metadata": {
        "id": "Aev1QO_ce1mY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Gymnasium: https://gymnasium.farama.org/\n",
        "\n",
        "Basic Usage: https://gymnasium.farama.org/content/basic_usage/\n",
        "\n",
        "Stable Baselines3: https://stable-baselines3.readthedocs.io/\n",
        "\n",
        "Stable Baselines3 RL Tutorial: https://github.com/araffin/rl-tutorial-jnrr19/tree/sb3\n",
        "\n",
        "RL Tips-And-Tricks: https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n"
      ],
      "metadata": {
        "id": "ZWAowKHfdqz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminaries"
      ],
      "metadata": {
        "id": "atVSHEUbkP2E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Software Installation\n",
        "\n",
        "Uses `pip` to install the Gymnasium environments (including Atari environments) and Stable Baselines 3 onto our cloud-hosted machine."
      ],
      "metadata": {
        "id": "ztBmmtCekTt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "# 1. Remove legacy gym (installed in Colab by default)\n",
        "!pip uninstall -y gym\n",
        "\n",
        "# environments\n",
        "! pip install \"gymnasium[box2d, atari, mujoco]\"\n",
        "\n",
        "! sudo apt install swig # needed for box2d\n",
        "\n",
        "# RL algorithms\n",
        "! pip install \"stable-baselines3[extra]\"\n",
        "\n",
        "# For visualization\n",
        "! apt-get update && apt-get install ffmpeg freeglut3-dev xvfb"
      ],
      "metadata": {
        "id": "GIrGP9jKiyHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "xmfnhCwKkX65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# primary package imports\n",
        "import stable_baselines3\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import re\n",
        "import base64\n",
        "\n",
        "from time import sleep\n",
        "from pathlib import Path\n",
        "\n",
        "# for pretty display in notebook\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from stable_baselines3.common import results_plotter\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "from stable_baselines3.common.callbacks import BaseCallback, CallbackList\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
        "\n",
        "# suppresses deprecation warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "# display library versions\n",
        "print(f'stable_baselines3 version: {stable_baselines3.__version__}')\n",
        "print(f'gym version: {gym.__version__}')"
      ],
      "metadata": {
        "id": "4dgJavJWi6Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking Runtime Configuration (CPU or GPU)"
      ],
      "metadata": {
        "id": "EOptweHMLyT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess, textwrap\n",
        "\n",
        "try:\n",
        "    out = subprocess.check_output([\"nvidia-smi\"], stderr=subprocess.STDOUT).decode()\n",
        "    print(\"Runtime hardware: GPU\")\n",
        "    print(textwrap.shorten(out.splitlines()[2], width=120))\n",
        "except Exception as e:\n",
        "    print(\"Runtime hardware: CPU-only\")"
      ],
      "metadata": {
        "id": "FQ4DtXLkL3-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Code"
      ],
      "metadata": {
        "id": "N32Tuij1OYXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dirs(dirs):\n",
        "  for dir in dirs:\n",
        "    if not os.path.exists(dir):\n",
        "      os.makedirs(dir)\n",
        "      print(f\"Created directory: {dir}\")\n",
        "\n",
        "def create_env_instance(env_name, log_dir=None, wrap=True, **env_kwargs):\n",
        "  env = gym.make(env_name, **env_kwargs)\n",
        "\n",
        "  # wraps the environment to provide additional functionality\n",
        "  if wrap:\n",
        "    env = Monitor(env, filename=log_dir)\n",
        "\n",
        "  return env\n",
        "\n",
        "def show_env_info(env):\n",
        "  spec = gym.spec(env.unwrapped.spec.id)\n",
        "\n",
        "  print(f'Environment Name: {spec.id}')\n",
        "  print(f'Action Space: {env.action_space}')\n",
        "  print(f'Observation Space: {env.observation_space}')\n",
        "  print(f'Max Episode Steps: {spec.max_episode_steps}')\n",
        "  print(f'Nondeterministic: {spec.nondeterministic}')\n",
        "\n",
        "def get_session_name(env, algorithm):\n",
        "    \"\"\"\n",
        "    Build a filesystem-friendly session name from an environment and an\n",
        "    SB3 algorithm instance.\n",
        "\n",
        "    Example output: \"CartPole-v1_PPO\"\n",
        "    \"\"\"\n",
        "    if isinstance(env, str):\n",
        "        env_name = env\n",
        "    else:\n",
        "        # handle vectorized envs (DummyVecEnv, SubprocVecEnv, etc.)\n",
        "        base_env = env\n",
        "        if hasattr(env, \"envs\") and len(env.envs) > 0:\n",
        "            base_env = env.envs[0]\n",
        "\n",
        "        # unwrap if needed (Monitor, TimeLimit, etc.)\n",
        "        base_env = getattr(base_env, \"unwrapped\", base_env)\n",
        "\n",
        "        # try spec.id, fall back to class name\n",
        "        spec = getattr(base_env, \"spec\", None)\n",
        "        env_name = getattr(spec, \"id\", None) or type(base_env).__name__\n",
        "\n",
        "    # get algorithm name from instance\n",
        "    alg_name = type(algorithm).__name__   # e.g., \"PPO\", \"TD3\", \"SAC\"\n",
        "\n",
        "    # combine and make filesystem-safe\n",
        "    raw_name = f\"{env_name}_{alg_name}\"\n",
        "    safe_name = re.sub(r\"[^A-Za-z0-9_.-]+\", \"_\", raw_name)\n",
        "\n",
        "    return safe_name\n",
        "\n",
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "    def __init__(self, check_freq: int, log_dir: str, save_dir: str, save_filename: str, verbose=1):\n",
        "        super().__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.log_dir = log_dir\n",
        "        self.save_dir = save_dir\n",
        "        self.save_path = os.path.join(save_dir, save_filename)\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "            # Retrieve training reward\n",
        "            x, y = results_plotter.ts2xy(\n",
        "                results_plotter.load_results(self.log_dir), \"timesteps\")\n",
        "            if len(x) > 0:\n",
        "                # Mean training reward over the last 100 episodes\n",
        "                mean_reward = np.mean(y[-100:])\n",
        "                if self.verbose > 0:\n",
        "                    clear_output(wait=True)\n",
        "\n",
        "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
        "                    print(\n",
        "                        f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\"\n",
        "                    )\n",
        "\n",
        "                # New best model, you could save the agent here\n",
        "                if mean_reward > self.best_mean_reward:\n",
        "                    self.best_mean_reward = mean_reward\n",
        "                    # Example for saving best model\n",
        "                    if self.verbose > 0:\n",
        "                        print(f\"Saving new best model to {self.save_path}.zip\")\n",
        "                    self.model.save(self.save_path)\n",
        "\n",
        "        return True\n",
        "\n",
        "def record_video(env_id, model, video_path, prefix=\"rl-video\", video_length=500):\n",
        "    eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
        "    eval_env = VecVideoRecorder(\n",
        "        eval_env,\n",
        "        video_folder=video_path,\n",
        "        record_video_trigger=lambda step: step == 0,\n",
        "        video_length=video_length,\n",
        "        name_prefix=prefix\n",
        "    )\n",
        "\n",
        "    obs = eval_env.reset()\n",
        "    for _ in range(video_length):\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "    eval_env.close()\n",
        "\n",
        "def show_videos(video_path, prefix=\"rl-video\"):\n",
        "    html = []\n",
        "    for mp4 in Path(video_path).glob(f\"{prefix}*.mp4\"):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append(\n",
        "            \"\"\"<video alt=\"{}\" autoplay\n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>\"\"\".format(\n",
        "                mp4, video_b64.decode(\"ascii\")\n",
        "            )\n",
        "        )\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "metadata": {
        "id": "eHAqyIGvOdrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Drive Configuration"
      ],
      "metadata": {
        "id": "2GxtVzFO3oDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because training can take a LONG time, you will want to save the models you learn to Google Drive as you go.\n",
        "\n",
        "If you have to take a break, you can load the model from Drive and start where you left off.\n",
        "\n",
        "This will require you to granting the notebook access to your Google Drive and creating a directory within Google Drive to save your current model."
      ],
      "metadata": {
        "id": "1qLtJLcjh3Qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mounts your Google Drive as a directory in your Colab virtual machine\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "Krrc3ECu3tQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creates the top-level directory where all Google Drive content will be saved\n",
        "drive_basedir='/content/drive/MyDrive/comp377/colab'\n",
        "os.makedirs(drive_basedir, exist_ok=True)"
      ],
      "metadata": {
        "id": "oS7lC-bAI4Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls $drive_basedir"
      ],
      "metadata": {
        "id": "vM0EnlcfMplL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Global Configuration"
      ],
      "metadata": {
        "id": "Cwgv-AYNQYb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# contains temporary files created during training\n",
        "train_dir = '/tmp/gym/model/train/'\n",
        "tensorboard_dir = '/tmp/gym/tensorboard'\n",
        "\n",
        "# directories that will contain content generated during training\n",
        "model_dir  = os.path.join(drive_basedir, 'models')\n",
        "video_dir  = os.path.join(drive_basedir, 'videos')\n",
        "figure_dir = os.path.join(drive_basedir, 'figures')\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(tensorboard_dir, exist_ok=True)\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "os.makedirs(video_dir, exist_ok=True)\n",
        "os.makedirs(figure_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "jd0b3VKGQep9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up fake display; otherwise video rendering will fail\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "metadata": {
        "id": "ENH_tdshSFXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environments\n",
        "\n"
      ],
      "metadata": {
        "id": "tLvSPBLPka-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A large number of pre-made environments are available to choose. (See [Gymnasium Docs](https://gymnasium.farama.org/))."
      ],
      "metadata": {
        "id": "RD2j38G-Yckj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the RL Algorithm\n",
        "\n"
      ],
      "metadata": {
        "id": "35T-FwTNlDJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stable-baselines 3 has a nice set of ready-to-use, modern [RL algorithms](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html).\n",
        "\n",
        "However, not all algorithms are suitable for every environment. Pay careful attention to the types of environments each algorithm supports (Box, Discrete, MultiDiscrete, MultiBinary, or Multi Processing)."
      ],
      "metadata": {
        "id": "mgzkmAaWYgP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import algorithms\n",
        "from stable_baselines3 import PPO, TD3, HER, A2C, SAC, DQN, DDPG"
      ],
      "metadata": {
        "id": "ZkBDk6JFuI1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training with CPU Only"
      ],
      "metadata": {
        "id": "VuV-PT8spLNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_id = 'CartPole-v1'"
      ],
      "metadata": {
        "id": "vOdSRuL16QH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an instance of the environment to use for training\n",
        "env = create_env_instance(env_id, train_dir, render_mode='rgb_array')\n",
        "show_env_info(env)"
      ],
      "metadata": {
        "id": "63r0xJHeBzmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be using the [Proximal Policy Optimization (PPO)](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) algorithm for this demonstration. The first argument to the PPO initializer is the **type of neural network** it will use to approximate the policy function.\n",
        "\n",
        "For this environment, we'll use a simple multilayer perceptron (`'MlpPolicy'`).\n",
        "\n",
        "When learning directly from pixels, such as we might do when training on an Atari environment, it would likely be better to use a convolutional neural network (CNN) based policy `'CnnPolicy'`."
      ],
      "metadata": {
        "id": "NZeG3yt9mLMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "algorithm = PPO('MlpPolicy', env, verbose=0, tensorboard_log=tensorboard_dir)"
      ],
      "metadata": {
        "id": "cOnzY69sw2X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# identifier for this training session based on environment and algorithm\n",
        "session_name = get_session_name(env, algorithm)"
      ],
      "metadata": {
        "id": "fZcjNY6mOEPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# identifier for this training session based on environment and algorithm\n",
        "session_name = get_session_name(env, algorithm)\n",
        "\n",
        "# change to true to load the model from disk\n",
        "load_model = False\n",
        "if load_model:\n",
        "  algorithm.load(f'{model_dir}/{session_name}.zip')"
      ],
      "metadata": {
        "id": "WUyynysZDwbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Model (Before Training)"
      ],
      "metadata": {
        "id": "sKldI6MxECrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_env = create_env_instance(env_id)\n",
        "mean_reward, std_reward = evaluate_policy(algorithm, eval_env, n_eval_episodes=25)\n",
        "\n",
        "print(f\"mean_reward (before training):{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "metadata": {
        "id": "DSsbYBriEKiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show result of \"random\" behaviors"
      ],
      "metadata": {
        "id": "aHwDkramSoUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "record_video(env_id, algorithm, '/tmp', video_length=500, prefix=session_name)\n",
        "show_videos('/tmp', session_name)"
      ],
      "metadata": {
        "id": "9ew8jJeGSaE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Callbacks\n",
        "\n",
        "\"A callback is a set of functions that will be called at given stages of the training procedure. You can use callbacks to access internal state of the RL model during training. It allows one to do monitoring, auto saving, model manipulation, progress bars, etc.\" (Read more about callbacks [here](https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html).)\n",
        "\n",
        "In this demonstration, we'll be using callbacks to monitor the average reward and save the learned `model`.\n",
        "\n",
        "The term \"model\" is used here to refer to whatever parameters and functions the algorithm learned during training, which are used to construct a policy. In most modern RL algorithms, the learned models include some flavor of neural networks.\n",
        "\n"
      ],
      "metadata": {
        "id": "lOA_rg_v4Oo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensorboard\n",
        "\n",
        "[Tensorboard](https://www.tensorflow.org/tensorboard) is a visualization tool that is broadly applicable to all types of machine learning. It is particularly useful for understanding what is going on during the training process and understanding how the optimization process affects the learned parameters (e.g., weights and biases in a neural network)."
      ],
      "metadata": {
        "id": "6DC5yjLxQlqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "dgBmBT4rWZ3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# launches a tensorboard that reads data saved in $tensorboard_dir that was\n",
        "# generated during the training session\n",
        "%tensorboard --logdir $tensorboard_dir"
      ],
      "metadata": {
        "id": "vkNRL3fQRj2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "p92quaT1ELsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'session_name: {session_name}')\n",
        "print(f'saving models to {model_dir}')"
      ],
      "metadata": {
        "id": "I8CAczt6PvgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_training_steps = 50_000\n",
        "n_steps_per_eval = 5_000\n",
        "\n",
        "# start training loop (periodically invoking callbacks)\n",
        "callback_list = CallbackList([\n",
        "  SaveOnBestTrainingRewardCallback(check_freq=n_steps_per_eval,\n",
        "                                   log_dir=train_dir,\n",
        "                                   save_dir=model_dir,\n",
        "                                   save_filename=session_name,\n",
        "                                   )\n",
        "])\n",
        "\n",
        "model = algorithm.learn(total_timesteps=n_training_steps,\n",
        "                        callback=callback_list)"
      ],
      "metadata": {
        "id": "QSduayQZmz-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot Training Curve"
      ],
      "metadata": {
        "id": "bQYW3Rp1qkYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_plotter.plot_results([train_dir],\n",
        "                             num_timesteps=n_training_steps,\n",
        "                             x_axis=results_plotter.X_TIMESTEPS,\n",
        "                             task_name=env.unwrapped.spec.id,\n",
        "                             figsize=(8,6))"
      ],
      "metadata": {
        "id": "KDh639uZqpsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Model (After Training)"
      ],
      "metadata": {
        "id": "bONmReBTlEyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_env = create_env_instance(env_id)\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=25)\n",
        "\n",
        "print(f\"mean_reward (after training):{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "metadata": {
        "id": "WQb0D7Blpr57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing Policy (After Training)"
      ],
      "metadata": {
        "id": "ml2ssDqkpdb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filenames for videos will be prefixed with the session name\n",
        "# (e.g., 'CartPole-v1-PPO-')\n",
        "record_video(env_id, model, video_dir, video_length=500, prefix=session_name)\n",
        "show_videos(video_dir, session_name)"
      ],
      "metadata": {
        "id": "-FdMQELfVBpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using the Trained Model"
      ],
      "metadata": {
        "id": "Bkf4mu3obSgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This environment uses the same Gymnasium interface that we used in lab3, so we\n",
        "can use our trained model in exactly the same way! (**Yay for consistent APIs!**)\n",
        "\n",
        "The environment has a `step()` method that takes an action and returns an `observation`, `reward`, and whether the environment has `terminated`.\n",
        "\n",
        "The `model` we trained earlier contains a policy, which we can use to select actions. In particular, `model.predict()` will return an action for the current environment `observation`."
      ],
      "metadata": {
        "id": "xqQksCYGUAqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run(env_id, model, delay=0.1):\n",
        "  eval_env = gym.make(env_id, render_mode=\"human\")\n",
        "\n",
        "  obs, info = eval_env.reset()\n",
        "\n",
        "  terminated, truncated = False, False\n",
        "  t = 1\n",
        "\n",
        "  while not (terminated or truncated):\n",
        "      action, _ = model.predict(obs, deterministic=True)\n",
        "      obs, reward, terminated, truncated, info = eval_env.step(action)\n",
        "\n",
        "      print(f't: {t}')\n",
        "      print(f'observation: {obs}')\n",
        "      print(f'selected action: {action}')\n",
        "      print(f'reward: {reward}')\n",
        "      print(f'terminated: {terminated}')\n",
        "      print(f'truncated: {truncated}')\n",
        "\n",
        "      clear_output(wait=True)\n",
        "      if delay > 0:\n",
        "        sleep(delay)\n",
        "\n",
        "      t += 1"
      ],
      "metadata": {
        "id": "ih964iZWXUL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run(env_id, model)"
      ],
      "metadata": {
        "id": "PJNMCwrYa-8n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}